{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on the [AI for beginners](https://github.com/microsoft/AI-For-Beginners) course from microsoft and uses the same framework to build the network, although the derivatives are calculated in a different way, following my own mathematical resolution of an N-layered perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a perceptron framework we need 3 elements, in the form of classes:\n",
    "\n",
    "- Hidden layer\n",
    "- Output layer\n",
    "- Stackable network framework\n",
    "\n",
    "Both layers have a linear transformation forward pass, which outputs a transformation based on the weights and bias of the model. Then the hidden layer has an activation function forward pass which applies whatever activation function you choose to the layer, and the output layer has an activation function forward pass which is always a softmax to output probabilities. \n",
    "Both of them have a backward pass to calculate the derivatives of the weights and bias and an update method to update them.\n",
    "\n",
    "The stackable network framework is the framework that allows us to stack multiple hidden layers and an output layer in our perceptron model and perform forward and backward passes on all of the layers automatically. \n",
    "\n",
    "The loss function is a cross entropy function and the output layer uses a softmax activation function, since that is what I assumed for my mathematical resolution, but any hidden layer activation function can be used. In particular I will be using tanh since it is a simple function to differentiate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector batches for SGD\n",
    "\n",
    "MNIST digits are arrays of shape (784,)\n",
    "\n",
    "To use SGD we will use a batch of input vectors instead of a single one. A batch of vectors is an array of shape (b,784)\n",
    "\n",
    "The reasoning was done with column vectors (784,) so this new approach requires us to transpose the matrix multiplications.\n",
    "This involves changing the order and then transposing both vector and matrix. (the vector is already transposed)\n",
    "\n",
    "By changing the order of all terms and transposing the matrix at __init__ (changing input and output dimensions) we achieve\n",
    "the same matrix multiplications, now with vectors of shape (,784) for forward and backward passes. \n",
    "\n",
    "Now that we have the multiplication for row vectors, we have to accomodate for a batch of vectors. The result of a linear \n",
    "transformation of a batch is a matrix, where every row is the result of the transformation of one vector of the batch. Now the bias needs to be a vector of shape (1,nout), and numpy understands to add it in every row of the resulting matrix since it is a row vector.\n",
    "\n",
    "Vector-vector multiplication when changing to batch matrix-batch matrix multiplication is simply the sum of all pairs of vector vector multiplication. \n",
    "If the batch has 4 vectors, instead of (1,784) each vector is a matrix (4,784) and their multiplication is a sum over all \n",
    "4 pairs of vector-vector multiplications. \n",
    "In the case of the bias, they don't sum \n",
    "themselves, the derivative of the bias is a (b,...) matrix so we must sum over axis=0 to obtain the proper shape.\n",
    "\n",
    "This means that all operations can stay the same for a batch-batch multiplication as they were for a single vector when doing forward and backward passes.\n",
    "\n",
    "\n",
    "In a batch of b vectors, the dimensions of the vectors for layer n are\n",
    "\n",
    "Forward\n",
    "    - input vector   (b,m(n-1))\n",
    "    - Weights        (m(n-1),m(n))\n",
    "    - bias           (1,m(n))\n",
    "    - output         (b,m(n))\n",
    "\n",
    "Backward\n",
    "    - derivative activation  (b,m(n))\n",
    "    - A coefficient          (b,m(n-1))\n",
    "    - A \"previous\"(n+1)      (b,m(n))\n",
    "    - Weights derivatives    (m(n-1),m(n))\n",
    "    - bias derivatives       (b,m(n-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "#from sklearn.datasets import make_classification\n",
    "# pick the seed for reproducibility - change it to explore the effects of random variations\n",
    "np.random.seed(0)\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as mnist_pickle:\n",
    "    #MNIST = pickle.load(mnist_pickle,encoding='latin1')\n",
    "    training_data, validation_data, test_data = pickle.load(mnist_pickle,encoding='latin1')\n",
    "#MNIST = pd.read_pickle('mnist.pkl.gz',compression='gzip')\n",
    "\n",
    "# Split into train and test dataset\n",
    "#train_x, test_x = np.split(MNIST[0], [n*8//10])\n",
    "#train_labels, test_labels = np.split(Y, [n*8//10])\n",
    "\n",
    "#MNIST is a tuple containing already a set of 50000 (train) and 2 sets of 10000 (validation and test)\n",
    "#MNIST[i][j][k]\n",
    "    #i indicates the train 0 validation 1 test 2\n",
    "    #j indicates array of digits 0 or labels 1\n",
    "    #k indicates which digit or label 0-49999 0-9999\n",
    "\n",
    "#training_data[i][j]\n",
    "    #i indicates array of digits 0 or labels 1\n",
    "    #j indicates which digit or label 0-49999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer():\n",
    "    def __init__(self,input_dimension,output_dimension):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (input_dimension,output_dimension))\n",
    "        self.b = np.zeros((1,output_dimension))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def linear_forward(self,x):\n",
    "        self.yn_1 = x                       #storing y(n-1) for later\n",
    "        return np.dot(x,self.W) + self.b    #returns the linear transformation\n",
    "    \n",
    "    def activation_forward(self,z):\n",
    "        y = np.tanh(z)\n",
    "        self.da = 1-y*y                     #storing a'(n) for layer\n",
    "        return y                            #tanh squishes values between -1 and 1\n",
    "    \n",
    "    def backward(self,A_prev):\n",
    "        \n",
    "        Ada = np.multiply(A_prev,self.da)\n",
    "        \n",
    "        self.dW = np.matmul(self.yn_1.T,Ada)\n",
    "        self.db = Ada.sum(axis=0)\n",
    "\n",
    "        return np.matmul(Ada,self.W.T)\n",
    "        \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output_Layer():\n",
    "    def __init__(self,input_dimension,output_dimension):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (input_dimension,output_dimension))\n",
    "        self.b = np.zeros((1,output_dimension))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def linear_forward(self,x):\n",
    "        self.yn_1 = x                       #storing y(n-1) for later\n",
    "        return np.dot(x,self.W) + self.b    #returns the linear transformation\n",
    "    \n",
    "    def activation_forward(self,z):        \n",
    "        #adding a constant >1 in front of z makes small increases in z bring greater increases in p\n",
    "        zmax = z.max(axis=1,keepdims=True) #this prevents overflow for big z values and does not change p\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        self.p = expz / Z                   #storing p for the backward pass\n",
    "        return self.p\n",
    "\n",
    "    \n",
    "    def backward(self,labels):\n",
    "        \n",
    "        #labels is vector of shape (b,)\n",
    "        \n",
    "        #I get p from self.p since I stored in the forward pass\n",
    "        \n",
    "        #p_of_f = \\sum_alpha -(d_gamma_i - p_i)    (b,m(N))\n",
    "        \n",
    "        #p is a matrix of shape (b,m(N)) and to transform it into p_of_f I need to iterate over every row based on the order\n",
    "        #of the labels -> 0th label 0th row, 1st label 1st row\n",
    "        #and from every row transform the probability matching the label value into p_of_f = p-1\n",
    "        #the others stay as p_of_f = p\n",
    "        \n",
    "        p_of_f = self.p\n",
    "        for i,lab in enumerate(labels):\n",
    "            p_of_f[i,lab] -= 1\n",
    "        \n",
    "        self.db = p_of_f.sum(axis=0)\n",
    "        self.dW = np.matmul(self.yn_1.T,p_of_f)\n",
    "\n",
    "        return np.matmul(p_of_f,self.W.T)\n",
    "        \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,y):\n",
    "        for l in self.layers:\n",
    "            #y = l.linear_forward(y)\n",
    "            #y = l.activation_forward(y)\n",
    "            \n",
    "            y = l.activation_forward(l.linear_forward(y))\n",
    "\n",
    "        return y                 #returns p\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        #return z               #I don't need backward to return A(1)\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single layer perceptron\n",
    "\n",
    "The first test we can do is create a simple 1 layer network and train it with a number of batches that covers our training set exactly once. This is sometimes known as a training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy for training data:  0.06678\n",
      "Final accuracy for training data:  0.90942\n"
     ]
    }
   ],
   "source": [
    "training_x = training_data[0]\n",
    "training_labels = training_data[1]\n",
    "\n",
    "net = Network()\n",
    "#first layer gets training_x.shape[1] as input, output layer gets max(training_labels)+1 as output\n",
    "net.add(Output_Layer(training_x.shape[1],max(training_labels)+1))\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "\n",
    "pred = np.argmax(net.forward(training_x),axis=1)\n",
    "acc = (pred==training_labels).mean()\n",
    "print(\"Initial accuracy for training data: \",acc)\n",
    "\n",
    "for i in range(0,len(training_x),batch_size):\n",
    "    xb = training_x[i:i+batch_size]\n",
    "    yb = training_labels[i:i+batch_size]\n",
    "    # forward pass\n",
    "    p = net.forward(xb)\n",
    "    # backward pass\n",
    "    net.backward(yb)\n",
    "    net.update(learning_rate)\n",
    "    \n",
    "\n",
    "pred = np.argmax(net.forward(training_x),axis=1)\n",
    "acc = (pred==training_labels).mean()\n",
    "print(\"Final accuracy for training data: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for testing data:  0.9089\n"
     ]
    }
   ],
   "source": [
    "testing_x = test_data[0]\n",
    "testing_labels = test_data[1]\n",
    "\n",
    "pred = np.argmax(net.forward(testing_x),axis=1)\n",
    "acc = (pred==testing_labels).mean()\n",
    "print(\"Accuracy for testing data: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single layer perceptron as we did in our previous project, and we can see that now our accuracy is much higher, going from approximately 0.7 to arround 0.9 just by changing the mathematical operations the layer performs to calculate probabilities and its training, but we can still improve by adding more layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multi layered perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy for training data:  0.11936\n",
      "Final accuracy for training data:  0.95082\n"
     ]
    }
   ],
   "source": [
    "training_x = training_data[0]\n",
    "training_labels = training_data[1]\n",
    "\n",
    "net = Network()\n",
    "#first layer gets training_x.shape[1] as input, output layer gets max(training_labels)+1 as output\n",
    "#adding a layer with 10 nodes\n",
    "#the output dimension for one layer and input dimension for the next must be the same\n",
    "m = 60\n",
    "n = 60\n",
    "net.add(Hidden_Layer(training_x.shape[1],m))\n",
    "net.add(Hidden_Layer(m,n))\n",
    "net.add(Output_Layer(n,10))#max(training_labels)+1))\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "\n",
    "pred = np.argmax(net.forward(training_x),axis=1)\n",
    "acc = (pred==training_labels).mean()\n",
    "print(\"Initial accuracy for training data: \",acc)\n",
    "\n",
    "for i in range(0,len(training_x),batch_size):\n",
    "    xb = training_x[i:i+batch_size]\n",
    "    yb = training_labels[i:i+batch_size]\n",
    "    # forward pass\n",
    "    p = net.forward(xb)\n",
    "    # backward pass\n",
    "    net.backward(yb)\n",
    "    net.update(learning_rate)\n",
    "    \n",
    "pred = np.argmax(net.forward(training_x),axis=1)\n",
    "acc = (pred==training_labels).mean()\n",
    "print(\"Final accuracy for training data: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for testing data:  0.9461\n"
     ]
    }
   ],
   "source": [
    "testing_x = test_data[0]\n",
    "testing_labels = test_data[1]\n",
    "\n",
    "pred = np.argmax(net.forward(testing_x),axis=1)\n",
    "acc = (pred==testing_labels).mean()\n",
    "print(\"Accuracy for testing data: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the accuracy went up a bit from 0.9 to 0.94 but not a great effect. The amount of nodes in the hidden layers play a very significant role, for example trying it with 5, 10, 20, 40, 60, 80, 100 nodes will show a difference probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a grid search function for maybe learning rate, size of hidden layers and maybe number of hidden layers\n",
    "\n",
    "#Compare performance with AI for beginners' method \n",
    "\n",
    "#Make a class named perceptron that takes as input a list of the number of nodes of every layer and creates a multi layer \n",
    "#network\n",
    "#has a method named train that takes as input training data, labels, learning rate, batch size and basically does the same as\n",
    "#the cell above\n",
    "#has a method called accuracy that takes as input testing data, testing labels and returns accuracy. \n",
    "\n",
    "#also add confusion matrix and most mislabeled pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(layer_nodes_list):\n",
    "    #Makes a Network class object named perceptron using a list with the number of nodes in every layer, including input layer\n",
    "    perceptron = Network()\n",
    "    if len(layer_nodes_list) > 2:\n",
    "        for i in range(len(layer_nodes_list)-2):\n",
    "            perceptron.add(Hidden_Layer(layer_nodes_list[i],layer_nodes_list[i+1]))\n",
    "        perceptron.add(Output_Layer(layer_nodes_list[-2],layer_nodes_list[-1]))\n",
    "    elif len(layer_nodes_list) == 2:\n",
    "        perceptron.add(Output_Layer(layer_nodes_list[0],layer_nodes_list[1]))\n",
    "    else:\n",
    "        print(\"Specify at least the number of nodes of the input and output layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self,layer_nodes_list):  \n",
    "        #At inicialization crates an attribute named model that contains a Network class object created using a list with the \n",
    "        #number of nodes in every layer, including input layer\n",
    "        self.model = Network()\n",
    "        if len(layer_nodes_list) > 2:\n",
    "            for i in range(len(layer_nodes_list)-2):\n",
    "                self.model.add(Hidden_Layer(layer_nodes_list[i],layer_nodes_list[i+1]))\n",
    "            self.model.add(Output_Layer(layer_nodes_list[-2],layer_nodes_list[-1]))\n",
    "        elif len(layer_nodes_list) == 2:\n",
    "            self.model.add(Output_Layer(layer_nodes_list[0],layer_nodes_list[1]))\n",
    "        else:\n",
    "            print(\"Specify at least the number of nodes of the input and output layers\")\n",
    "    \n",
    "    def train(self,training_x,training_labels,learning_rate,batch_size,n_epochs=1): \n",
    "        #performs n training epochs\n",
    "        for n in range(n_epochs):\n",
    "            for i in range(0,len(training_x),batch_size):\n",
    "                xb = training_x[i:i+batch_size]\n",
    "                yb = training_labels[i:i+batch_size]\n",
    "                # forward pass\n",
    "                p = self.model.forward(xb)\n",
    "                # backward pass\n",
    "                self.model.backward(yb)\n",
    "                self.model.update(learning_rate)\n",
    "    \n",
    "    def accuracy(self,testing_x,testing_labels):\n",
    "        pred = np.argmax(self.model.forward(testing_x),axis=1)\n",
    "        acc = (pred==testing_labels).mean()\n",
    "        print(\"Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45092272, -0.02497476,  0.07611022, -0.42396774],\n",
       "       [-0.78618913, -0.1834337 , -0.07704386, -0.16546929],\n",
       "       [ 0.2673952 , -0.95011059,  1.16504341, -0.0733936 ]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = Perceptron([3,4,6,9,4,6,3])\n",
    "perc.model.layers[0].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch = 5\n",
    "#nepoch = 3 add at the end if needed\n",
    "perc = Perceptron([784,30,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc.train(training_data[0],training_data[1],lr,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9411\n"
     ]
    }
   ],
   "source": [
    "perc.accuracy(test_data[0],test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AI for beginners' method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a perceptron framework we need 5 elements, which will be in the form of classes:\n",
    "\n",
    "- linear transformation\n",
    "- hidden layer activation function\n",
    "- softmax activation function\n",
    "- loss function ??????????????????????????\n",
    "- stackable network framework\n",
    "\n",
    "All element except the network framework have a forward pass to calculate the probabilities of the classes and a backward pass to calculate the derivatives of the weights through backpropagation. \n",
    "The loss function is a cross entropy function and the output layer uses a softmax activation function, since that is what I assumed for my mathematical resolution, but any hidden layer activation function can be used. In particular I will be using tanh since it is a simple function to differentiate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.88184657 -0.74112329 -0.16958856]\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self,input_dimension,output_dimension):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (output_dimension,input_dimension))\n",
    "        self.b = np.zeros((output_dimension))        \n",
    "\n",
    "    def forward(self,x): \n",
    "        return np.dot(self.W,x) + self.b \n",
    "\n",
    "    def backward(self):\n",
    "        return \n",
    "    \n",
    "x = np.array([1,-2,0,-1,2])\n",
    "lll = Linear(5,3)\n",
    "print(lll.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23412166 0.01165623 0.08612854 0.03168492 0.63640865]\n"
     ]
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        expz = np.exp(z)\n",
    "        Z = expz.sum(keepdims=True)\n",
    "        self.p = expz / Z\n",
    "        return self.p\n",
    "\n",
    "    def backward(self):\n",
    "        return 1\n",
    "\n",
    "softm = Softmax()\n",
    "print(softm.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76159416, -0.96402758,  0.        , -0.76159416,  0.96402758])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Activation_function:\n",
    "    def forward(self,z):\n",
    "        self.y = np.tanh(z)\n",
    "        return self.y #tanh squishes values between -1 and 1 \n",
    "\n",
    "activ = Activation_function()\n",
    "activ.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2655141655417551"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class loss_function:\n",
    "    def forward(self,p,labels):\n",
    "        p_of_true = p[np.arange(len(labels)), labels]\n",
    "        return -np.log(p_of_true).mean()\n",
    "\n",
    "crossloss = loss_function()\n",
    "p = np.array([[0.01,0.1,0.7,0.1,0.05,0.04],[0.84,0.01,0.01,0.05,0.05,0.04]])\n",
    "l = np.array([2,0])\n",
    "crossloss.forward(p,l) \n",
    "\n",
    "#Might be better as a function since I am not calling it to calculate derivatives only the loss itself, so it gets called only \n",
    "#once at the end of the training epoch or training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import gridspec\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "# pick the seed for reproducibility - change it to explore the effects of random variations\n",
    "np.random.seed(0)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X, Y = make_classification(n_samples = n, n_features=2,\n",
    "                           n_redundant=0, n_informative=2, flip_y=0.2)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "# Split into train and test dataset\n",
    "train_x, test_x = np.split(X, [n*8//10])\n",
    "train_labels, test_labels = np.split(Y, [n*8//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy:  0.2125\n",
      "Final accuracy:  0.825\n"
     ]
    }
   ],
   "source": [
    "#in progress\n",
    "net = Network()\n",
    "net.add(Output_Layer(2,2))\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "pred = np.argmax(net.forward(train_x),axis=1)\n",
    "acc = (pred==train_labels).mean()\n",
    "print(\"Initial accuracy: \",acc)\n",
    "\n",
    "batch_size=4\n",
    "for i in range(0,len(train_x),batch_size):\n",
    "    xb = train_x[i:i+batch_size]\n",
    "    yb = train_labels[i:i+batch_size]\n",
    "    \n",
    "    # forward pass\n",
    "    #z = lin.forward(xb)\n",
    "    #p = softmax.forward(z)\n",
    "    #loss = cross_ent_loss.forward(p,yb)\n",
    "    \n",
    "    p = net.forward(xb)\n",
    "    \n",
    "    \n",
    "    # backward pass\n",
    "    net.backward(yb)\n",
    "    net.update(learning_rate)\n",
    "    #dp = cross_ent_loss.backward(loss)\n",
    "    #dz = softmax.backward(dp)\n",
    "    #dx = lin.backward(dz)\n",
    "    #lin.update(learning_rate)\n",
    "    \n",
    "pred = np.argmax(net.forward(train_x),axis=1)\n",
    "acc = (pred==train_labels).mean()\n",
    "print(\"Final accuracy: \",acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition in progress for (784,) as input vector\n",
    "class Hidden_Layer_singlevec():\n",
    "    def __init__(self,input_dimension,output_dimension,batch):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (output_dimension,input_dimension))\n",
    "        self.b = np.zeros((1,output_dimension))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    def linear_forward(self,x):\n",
    "        self.yn_1 = x                       #storing y(n-1) for later\n",
    "        return np.dot(self.W,x) + self.b    #returns the linear transformation\n",
    "    \n",
    "    def activation_forward(self,z):\n",
    "        y = np.tanh(z)\n",
    "        self.da = 1-y*y                     #storing a'(n) for layer\n",
    "        return y                            #tanh squishes values between -1 and 1\n",
    "    \n",
    "    def backward(self,A_prev):\n",
    "        #calculate A of this layer\n",
    "        #AN1 = np.matmul(WN1.T,np.multiply(AN,a'N1))\n",
    "        \n",
    "        #calculate derivatives of this layer using An_1\n",
    "        #np.matmul(np.multiply(AN,a'N1),yN2.T)\n",
    "        #np.multiply(AN,a'N1)\n",
    "        \n",
    "        #store derivatives but not any A since they are fed as inputs\n",
    "        \n",
    "        #return A\n",
    "        \n",
    "        Ada = np.multiply(A_prev,self.da)\n",
    "        \n",
    "        A = np.matmul(self.W.T,Ada)\n",
    "        \n",
    "        dW = np.matmul(Ada,self.yn_1.T)\n",
    "        db = Ada\n",
    "        \n",
    "        #takes An as input\n",
    "        #calculates and stores derivatives\n",
    "        #returns An-1\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    def update(lr):\n",
    "        #the derivatives are already stored\n",
    "        #remember the - signs that can be removed\n",
    "        \n",
    "        #change W and b by derivatives no returning anything\n",
    "        \n",
    "        \n",
    "        \n",
    "hid = Hidden_Layer(5,3)\n",
    "print(1-hid.activation_forward(hid.linear_forward(x))*hid.activation_forward(hid.linear_forward(x)),hid.da)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
