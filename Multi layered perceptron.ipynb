{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. Take the framework code from the lesson and paste it into this notebook, or (even better) into a separate Python module\n",
    "1. Define and train one-layered perceptron, observing training and validation accuracy during training\n",
    "1. Try to understand if overfitting took place, and adjust layer parameters to improve accuracy\n",
    "1. Repeat previous steps for 2- and 3-layered perceptrons. Try to experiment with different activation functions between layers.\n",
    "1. Try to answer the following questions:\n",
    "    - Does the inter-layer activation function affect network performance?\n",
    "    - Do we need 2- or 3-layered network for this task?\n",
    "    - Did you experience any problems training the network? Especially as the number of layers increased.\n",
    "    - How do weights of the network behave during training? You may plot max abs value of weights vs. epoch to understand the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on the [AI for beginners](https://github.com/microsoft/AI-For-Beginners) course from microsoft and uses the same framework to build the network, although the derivatives are calculated in a different way, following my own mathematical resolution of an N-layered perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a perceptron framework we need 5 elements, which will be in the form of classes:\n",
    "\n",
    "- linear transformation\n",
    "- hidden layer activation function\n",
    "- softmax activation function\n",
    "- loss function ??????????????????????????\n",
    "- stackable network framework\n",
    "\n",
    "All element except the network framework have a forward pass to calculate the probabilities of the classes and a backward pass to calculate the derivatives of the weights through backpropagation. \n",
    "The loss function is a cross entropy function and the output layer uses a softmax activation function, since that is what I assumed for my mathematical resolution, but any hidden layer activation function can be used. In particular I will be using tanh since it is a simple function to differentiate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from sklearn.datasets import make_classification\n",
    "# pick the seed for reproducibility - change it to explore the effects of random variations\n",
    "np.random.seed(0)\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST digits are arrays of shape (784,)             (np.array([1,2,3,4]))\n",
    "#a batch of digits is an array of shape (n,784)\n",
    "#We will transpose them because the reasoning was done with column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97889949  0.21807771 -0.47248609]\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self,input_dimension,output_dimension):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (output_dimension,input_dimension))\n",
    "        self.b = np.zeros((output_dimension))\n",
    "        \n",
    "    def forward(self,x): \n",
    "        return np.dot(self.W,x) + self.b\n",
    "    \n",
    "    def backward(self):\n",
    "        return \n",
    "    \n",
    "lll = Linear(5,3)\n",
    "x = np.array([1,2,-1,0,1]) \n",
    "print(lll.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18719356 0.29192151 0.33898731 0.03704098 0.14485663] [0.18719356 0.29192151 0.33898731 0.03704098 0.14485663]\n"
     ]
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        expz = np.exp(z)\n",
    "        Z = expz.sum(keepdims=True)\n",
    "        self.p = expz / Z\n",
    "        return self.p\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.p\n",
    "softm = Softmax()\n",
    "print(softm.forward(x),softm.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76159416,  0.96402758, -0.76159416,  0.        ,  0.76159416])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Activation_function:\n",
    "    def forward(self,z):\n",
    "        self.y = np.tanh(z)\n",
    "        return self.y #tanh squishes values between -1 and 1\n",
    "    \n",
    "    \n",
    "activ = Activation_function()\n",
    "activ.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2655141655417551"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class loss_function:\n",
    "    def forward(self,p,labels):\n",
    "        p_of_true = p[np.arange(len(labels)), labels]\n",
    "        return -np.log(p_of_true).mean()\n",
    "crossloss = loss_function()\n",
    "p = np.array([[0.01,0.1,0.7,0.1,0.05,0.04],[0.84,0.01,0.01,0.05,0.05,0.04]])\n",
    "l = np.array([2,0])\n",
    "crossloss.forward(p,l) \n",
    "\n",
    "#Might be better as a function since I am not calling it to calculate derivatives only the loss itself, so it gets called only \n",
    "#once at the end of the training epoch or training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,y):\n",
    "        for l in self.layers:\n",
    "            y = l.forward(y)\n",
    "        return y\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b\n",
      "b c\n",
      "c d\n"
     ]
    }
   ],
   "source": [
    "Lis = ['a','b','c','d']\n",
    "for n,m in zip(Lis,Lis[1:]):\n",
    "    print(n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation using my method requires to calculate the A coefficients and then the derivatives\n",
    "\n",
    "#The A coefficients can be calculated backward pass, and are different for the Nth layer and the rest\n",
    "\n",
    "#For the rest, they require a matrix multiplication with the transposed weights of the same layer and the vector resulting from\n",
    "#an elemnt-wise multiplication of the previous A coefficient and the derivative of the activation function of the same layer. \n",
    "\n",
    "#In layer N, instead of the element-wise multiplication we get the probability of failure vector, which is a vector of the same\n",
    "#size as softmax output\n",
    "\n",
    "#The derivatives are also split between the Nth layer and the rest. \n",
    "\n",
    "#In the Nth layer they are calculated as a matrix multiplication of the probability of failure and the output of the N-1 layer \n",
    "#for the weights, and simply the probability of failure for the bias.\n",
    "\n",
    "#In the rest they are calculated as a matrix multiplication of the element-wise multiplication of the previous layer's A \n",
    "#coefficient and the derivative of the activation function of the same layer, and the output of the following layer. \n",
    "\n",
    "#So basically, softmax has its own backward pass since it is layer N, and the activation function has the recursive formula with\n",
    "#the element-wise multiplications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe redo the structure so the classes are only Hidden layer, Output layer and Network\n",
    "#Network is the same, just stacking layers\n",
    "\n",
    "#Hidden layer contains modules for a linear forward, activation forward, derivatives backward\n",
    "#Output layer contains modules for a linear forward, activation forward (same name as hidden but uses softmax), \n",
    "#update backward, to change weights of the layer. \n",
    "\n",
    "#Now a forward pass has half as many layers since instead of being separated into linear and activation they are together, but\n",
    "#in the loop you just do\n",
    "\n",
    "#x = layer.linear_forward(x)\n",
    "#x = layer.activation_forward(x)\n",
    "\n",
    "#or compose them x = layer.activation_forward(layer.linear_forward(x))\n",
    "\n",
    "#The benefits of this are that a single layer has access to both the weights w(n) the y(n-1) and the a'(n), which are needed to \n",
    "#calculate derivatives. The y(n-1) is also stored since it is the input used for a linar forward and as such is stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12\n",
      "24\n",
      "72\n",
      "144\n",
      "432\n",
      "864\n",
      "2592\n"
     ]
    }
   ],
   "source": [
    "V = 2 \n",
    "for n in range(4):\n",
    "    V = V*2\n",
    "    print(V)\n",
    "    V = V*3\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST digits are arrays of shape (784,)             (np.array([1,2,3,4]))\n",
    "#a batch of digits is an array of shape (n,784)\n",
    "#The reasoning was done with column vectors (784,) so this new approach requires us to transpose the matrix multiplications.\n",
    "#This involves changing the order and then transposing both vector and matrix. (vector is already transposed)\n",
    "\n",
    "#by changing the order of all terms and transposing the matrix at __init__ (changing input and output dimensions) we achieve\n",
    "#the same matrix multiplications, now with vectors of shape (,784)\n",
    "\n",
    "#Now that we have the multiplication for row vectors, we have to accomodate for multiple vectors. In the case of MNIST, multiple\n",
    "#vectors become a matrix of shape (n,784), and the result of a linear transformation is a matrix, where every row is the result\n",
    "#of the transformation of one vector of the batch. Now the a bias needs to be a vector of shape (1,nout), and numpy understands \n",
    "#to add it in every row of the resulting matrix since it is a row vector.\n",
    "\n",
    "#vector vector multiplication when changing to batch matrix is simply the sum of all pairs of vector vector multiplication. \n",
    "#If the batch has 4 vectors, instead of (1,784) each vector is a matrix (4,784) and their multiplication is a sum over all \n",
    "#4 pairs. \n",
    "\n",
    "#This means that all operations can stay the same when doing forward and backward passes, and that the derivatives will\n",
    "#automatically sum themselves when calculating them using a batch of vectors. \n",
    "\n",
    "#In a batch of b vectors\n",
    "\n",
    "#layer n\n",
    "#forward\n",
    "    #input vector   (b,m(n))\n",
    "    #Weights        (m(n-1),m(n))\n",
    "    #bias           (1,m(n))\n",
    "    #previous input (b,m(n-1))\n",
    "#backward\n",
    "    #derivative activation (b,m(n))\n",
    "    #A coefficient         (b,m(n-1))\n",
    "    #A \"previous\"(n+1)     (b,m(n))\n",
    "    #Weights derivatives   (m(n-1),m(n))\n",
    "    #bias derivatives      (b,m(n-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20406182 0.29158037 0.01810237] [0.20406182 0.29158037 0.01810237]\n"
     ]
    }
   ],
   "source": [
    "class Hidden_Layer():\n",
    "    def __init__(self,input_dimension,output_dimension,batch):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (output_dimension,input_dimension))\n",
    "        self.b = np.zeros((1,output_dimension))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    def linear_forward(self,x):\n",
    "        self.yn_1 = x                       #storing y(n-1) for later\n",
    "        return np.dot(x,self.W) + self.b    #returns the linear transformation\n",
    "    \n",
    "    def activation_forward(self,z):\n",
    "        y = np.tanh(z)\n",
    "        self.da = 1-y*y                     #storing a'(n) for layer\n",
    "        return y                            #tanh squishes values between -1 and 1\n",
    "    \n",
    "    def backward(self,A_prev):\n",
    "        #calculate A of this layer\n",
    "        #AN1 = np.matmul(WN1.T,np.multiply(AN,a'N1))\n",
    "        \n",
    "        #calculate derivatives of this layer using An_1\n",
    "        #np.matmul(np.multiply(AN,a'N1),yN2.T)\n",
    "        #np.multiply(AN,a'N1)\n",
    "        \n",
    "        #store derivatives but not any A since they are fed as inputs\n",
    "        \n",
    "        #return A\n",
    "        \n",
    "        Ada = np.multiply(A_prev,self.da)\n",
    "        \n",
    "        A = np.matmul(self.W.T,Ada)\n",
    "        \n",
    "        dW = np.matmul(Ada,self.yn_1.T)\n",
    "        db = Ada\n",
    "        \n",
    "        #takes An as input\n",
    "        #calculates and stores derivatives\n",
    "        #returns An-1\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    def update(lr):\n",
    "        #the derivatives are already stored\n",
    "        #remember the - signs that can be removed\n",
    "        \n",
    "        #change W and b by derivatives no returning anything\n",
    "        \n",
    "        \n",
    "        \n",
    "hid = Hidden_Layer(5,3)\n",
    "print(1-hid.activation_forward(hid.linear_forward(x))*hid.activation_forward(hid.linear_forward(x)),hid.da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition in progress for (784,) as input vector\n",
    "class Hidden_Layer_singlevec():\n",
    "    def __init__(self,input_dimension,output_dimension,batch):\n",
    "        self.W = np.random.normal(0,1.0/np.sqrt(input_dimension), (output_dimension,input_dimension))\n",
    "        self.b = np.zeros((1,output_dimension))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    def linear_forward(self,x):\n",
    "        self.yn_1 = x                       #storing y(n-1) for later\n",
    "        return np.dot(self.W,x) + self.b    #returns the linear transformation\n",
    "    \n",
    "    def activation_forward(self,z):\n",
    "        y = np.tanh(z)\n",
    "        self.da = 1-y*y                     #storing a'(n) for layer\n",
    "        return y                            #tanh squishes values between -1 and 1\n",
    "    \n",
    "    def backward(self,A_prev):\n",
    "        #calculate A of this layer\n",
    "        #AN1 = np.matmul(WN1.T,np.multiply(AN,a'N1))\n",
    "        \n",
    "        #calculate derivatives of this layer using An_1\n",
    "        #np.matmul(np.multiply(AN,a'N1),yN2.T)\n",
    "        #np.multiply(AN,a'N1)\n",
    "        \n",
    "        #store derivatives but not any A since they are fed as inputs\n",
    "        \n",
    "        #return A\n",
    "        \n",
    "        Ada = np.multiply(A_prev,self.da)\n",
    "        \n",
    "        A = np.matmul(self.W.T,Ada)\n",
    "        \n",
    "        dW = np.matmul(Ada,self.yn_1.T)\n",
    "        db = Ada\n",
    "        \n",
    "        #takes An as input\n",
    "        #calculates and stores derivatives\n",
    "        #returns An-1\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    def update(lr):\n",
    "        #the derivatives are already stored\n",
    "        #remember the - signs that can be removed\n",
    "        \n",
    "        #change W and b by derivatives no returning anything\n",
    "        \n",
    "        \n",
    "        \n",
    "hid = Hidden_Layer(5,3)\n",
    "print(1-hid.activation_forward(hid.linear_forward(x))*hid.activation_forward(hid.linear_forward(x)),hid.da)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
